---
license: cc-by-nc-nd-4.0
task_categories:
- question-answering
- summarization
- text-classification
dataset_info:
  features:
  - name: title
    dtype: string
  - name: date
    dtype: string
  - name: president
    dtype: string
  - name: url
    dtype: string
  - name: question_order
    dtype: int64
  - name: interview_question
    dtype: string
  - name: interview_answer
    dtype: string
  - name: gpt3.5_summary
    dtype: string
  - name: gpt3.5_prediction
    dtype: string
  - name: question
    dtype: string
  - name: annotator_id
    dtype: string
  - name: annotator1
    dtype: string
  - name: annotator2
    dtype: string
  - name: annotator3
    dtype: string
  - name: inaudible
    dtype: bool
  - name: multiple_questions
    dtype: bool
  - name: affirmative_questions
    dtype: bool
  - name: index
    dtype: int64
  - name: clarity_label
    dtype: string
  - name: evasion_label
    dtype: string
  splits:
  - name: train
    num_bytes: 14890155
    num_examples: 3448
  - name: test
    num_bytes: 768212
    num_examples: 308
  download_size: 4156940
  dataset_size: 15658367
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
---

## "I Never Said That": A dataset, taxonomy and baselines on response clarity classification.

This repo contains the dataset form the paper: ["I Never Said That": A dataset, taxonomy and baselines on response clarity classification](https://aclanthology.org/2024.findings-emnlp.300/). It contains question-answer (QA) pairs extracted from political interviews, with annotations that reflect varying degrees of response clarity or evasion. Below is a brief explanation of each column in the dataset.

![alt text](https://github.com/konstantinosftw/Question-Evasion/blob/main/logo.png?raw=true)


| Column                  | Description                                                                                                                                                                                                                                            |
|-------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **title**               | (Optional) Short descriptive title of the interview or event where the QA pair was recorded. Often remains empty if the information is not available.                                                                                                 |
| **date**                | (Optional) Date of the interview or event in a standardized format, if available.                                                                                                                                                                     |
| **president**           | (Optional) Name or identifier of the president (or key figure) involved in the interview, if relevant to the data.                                                                                                                                    |
| **url**                 | Link to the source of the interview or transcript.                                                                                                                                                                                                    |
| **interview_question**  | The full original question(s) posed by the interviewer. Sometimes, the interviewer asks multiple questions in one go (e.g., “What’s your name and where do you live?”).                                                                                |
| **interview_answer**    | The full text of the interviewee’s response to the question(s) above. This may include multiple sentences, clarifications, or tangential remarks.                                                                                                      |
| **gpt3.5_summary**      | (Optional) A short summary of the answer, generated by GPT-3.5 (if applicable). Often empty if the summary is not provided.                                                                                                                            |
| **gpt3.5_prediction**   | (Optional) A predicted clarity or evasion label for the response, generated by GPT-3.5 (if applicable). Often empty if not provided.                                                                                                                    |
| **question**            | A **sub-question** extracted from the original `interview_question`. If the interviewer asked multiple questions, each sub-question is isolated here (e.g., “What’s your name?”). The corresponding **label** focuses on how well the answer addresses this sub-question. |
| **clarity_label**       | **High-level hierarchical label** from the two-level taxonomy describing the response type: "Clear Reply", "Clear Non-Reply", or "Ambivalent Reply". |
| **evasion_label** | **Fine-grained sub-category label** from the second level of the hierarchical taxonomy. Values include: "Explicit", "Implicit", "Dodging", "General", "Deflection", "Partial/half-answer", "Declining to answer", "Claims ignorance", and "Clarification." |
| **annotator_id**        | (Optional) An identifier for the human annotator who labeled this QA pair. Can be empty if not recorded.                                                                                                                                              |
| **annotator1**          | (Optional) Additional label or feedback from a first annotator in a multi-annotator setup. Useful for capturing disagreements or consensus-building.                                                                                                   |
| **annotator2**          | (Optional) Additional label or feedback from a second annotator.                                                                                                                                                                                     |
| **annotator3**          | (Optional) Additional label or feedback from a third annotator.                                                                                                                                                                                      |
| **inaudible**           | Boolean flag (`True`/`False`) indicating whether part of the answer was inaudible or missing from the transcript.                                                                                                                                     |
| **multiple_questions**  | Boolean flag (`True`/`False`) indicating whether the `interview_question` contained multiple sub-questions.                                                                                                                                           |
| **affirmative_questions** | Boolean flag (`True`/`False`) indicating whether the question was formulated in an affirmative way (e.g., “Wouldn’t you agree…?”).                                                                                                                  |


The taxonomy from which the clarity and evasion labels are derived is illustrated below.

![Taxonomy Figure](https://raw.githubusercontent.com/konstantinosftw/Question-Evasion/refs/heads/main/taxonomy.png)
*Figure 3: The taxonomy of response clarity classification (from <a href="https://aclanthology.org/2024.findings-emnlp.300/" target="_blank">Thomas et al., 2024</a>).*


The taxonomy follows a two-level hierarchical structure where:
- **Clarity labels** represent the top-level categories: Clear Reply, Clear Non-Reply, and Ambivalent Reply
- **Evasion labels** correspond to the fine-grained subcategories within each clarity category

## Dataset Overview

- **Splits**: The dataset is divided into **train** and **test** sets.
- **Num Rows (train)**: `3448`  
- **Num Rows (test)**: `308`  

Each split has the same set of columns (as shown above), ensuring consistent features across the entire dataset.

## Source Paper (Abstract Excerpt)

> Equivocation and ambiguity in public speech are well-studied discourse phenomena, especially in political science and analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a corresponding clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided for a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level)...

For more details about the methodology, annotation process, and baseline experiments, please refer to the paper:


**["I Never Said That": A dataset, taxonomy and baselines on response clarity classification](https://aclanthology.org/2024.findings-emnlp.300/)**


### Instructions for Usage


Utilizing the dataset is a straightforward process. Import it into your Python environment using the following code:

```python 
from datasets import load_dataset
dataset = load_dataset("ailsntua/QEvasion")
```
The structure of the dataset is outlined below:

```python
DatasetDict({
    train: Dataset({
        features: ['title', 'date', 'president', 'url', 'interview_question', 'interview_answer', 'gpt3.5_summary', 'gpt3.5_prediction', 'question', 'label', 'annotator_id', 'annotator1', 'annotator2', 'annotator3', 'inaudible', 'multiple_questions', 'affirmative_questions'],
        num_rows: 3448
    })
    test: Dataset({
        features: ['title', 'date', 'president', 'url', 'interview_question', 'interview_answer', 'gpt3.5_summary', 'gpt3.5_prediction', 'question', 'label', 'annotator_id', 'annotator1', 'annotator2', 'annotator3', 'inaudible', 'multiple_questions', 'affirmative_questions'],
        num_rows: 308
    })
})
```
This dataset is presented as a simple CSV file containing annotations for each sub-question within the interview.

### Citation 

>@inproceedings{thomas-etal-2024-never,
    title = "{''}{I} Never Said That{''}: A dataset, taxonomy and baselines on response clarity classification",
    author = "Thomas, Konstantinos  and
      Filandrianos, Giorgos  and
      Lymperaiou, Maria  and
      Zerva, Chrysoula  and
      Stamou, Giorgos",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.300",
    doi = "10.18653/v1/2024.findings-emnlp.300",
    pages = "5204--5233",
    abstract = "Equivocation and ambiguity in public speech are well-studied discourse phenomena, especially in political science and analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a corresponding clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided for a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level). We combine ChatGPT and human annotators to collect, validate and annotate discrete QA pairs from political interviews, to be used for our newly introduced response clarity task. We provide a detailed analysis and conduct several experiments with different model architectures, sizes and adaptation methods to gain insights and establish new baselines over the proposed dataset and task.",
}